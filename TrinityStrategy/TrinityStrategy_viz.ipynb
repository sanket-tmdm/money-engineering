{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrinityStrategy Indicator Analysis Dashboard\n",
    "\n",
    "This notebook provides **automated** comprehensive analysis and visualization for **all instruments** configured in `uout.json`.\n",
    "\n",
    "## Features:\n",
    "- **Automated Multi-Instrument Processing**: Reads markets/securities from `uout.json` and processes all instruments\n",
    "- **Pattern 2 Connection Reuse**: Efficient single-connection approach for fetching multiple instruments\n",
    "- **Organized Output Structure**: Charts saved to `tier-1_output/charts/<instrument>/`\n",
    "- **Three Chart Types Per Instrument**:\n",
    "  - **Scout Time Series**: ADX, DI+, DI-, Bollinger Bands, Conviction Oscillator\n",
    "  - **Distribution Analysis**: Histograms of all Scout indicator values\n",
    "  - **Correlation Matrix**: Relationships between Scout indicators\n",
    "\n",
    "## Scout Indicators Visualized:\n",
    "- **TrendScout**: ADX, DI+, DI- (regime detection - trending vs ranging)\n",
    "- **TensionScout**: Bollinger Bands (volatility and overbought/oversold)\n",
    "- **CrowdScout**: Conviction Oscillator (volume confirmation)\n",
    "\n",
    "## Usage:\n",
    "1. Update `TOKEN`, `START_DATE`, and `END_DATE` in the configuration cell\n",
    "2. Run all cells sequentially\n",
    "3. Charts will be automatically generated for all instruments in `tier-1_output/charts/`\n",
    "\n",
    "**Note**: No need to specify individual markets/commodities - they're auto-parsed from `uout.json`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import asyncio\n",
    "import warnings\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import svr3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('üìä TrinityStrategy Indicator Dashboard Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5lwcfhe0v9m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for automation\n",
    "import json\n",
    "import os\n",
    "\n",
    "def parse_uout_instruments():\n",
    "    \"\"\"Parse uout.json to get all (market, security) pairs for visualization\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: [(market, code), ...] e.g., [('DCE', 'i<00>'), ('SHFE', 'cu<00>'), ...]\n",
    "    \"\"\"\n",
    "    with open('uout.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Access nested structure: uout.json has \"private\" as top-level key\n",
    "    private_config = config['private']\n",
    "    \n",
    "    markets = private_config['markets']  # e.g., ['DCE', 'SHFE']\n",
    "    securities_per_market = private_config['securities']  # e.g., [['i'], ['cu', 'sc']]\n",
    "    \n",
    "    # Build instrument list: [(market, code<00>), ...]\n",
    "    instruments = []\n",
    "    for market, securities in zip(markets, securities_per_market):\n",
    "        for security in securities:\n",
    "            # Append logical contract code (e.g., 'i<00>')\n",
    "            instruments.append((market, f'{security}<00>'))\n",
    "    \n",
    "    return instruments\n",
    "\n",
    "print('‚úì Config parser and automation utilities loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288fa082",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Update these settings to match your test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9483de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server configuration\n",
    "RAILS_URL = 'https://10.99.100.116:4433/private-api/'\n",
    "WS_URL = 'wss://10.99.100.116:4433/tm'\n",
    "TM_MASTER = ('10.99.100.116', 6102)\n",
    "\n",
    "TOKEN = '58abd12edbde042536637bfba9d20d5faf366ef481651cdbb046b1c3b4f7bf7a97ae7a2e6e5dc8fe05cd91147c8906f8a82aaa1bb1356d8cb3d6a076eadf5b5a' \n",
    "\n",
    "# Date range matching your quick test (7 days: Oct 25 - Nov 1, 2024)\n",
    "START_DATE = 20240101000000\n",
    "END_DATE = 20250101000000\n",
    "\n",
    "# Indicator configuration\n",
    "INDICATOR_NAME = 'TrinityStrategy'\n",
    "GRANULARITY = 900  # 15-minute bars\n",
    "NAMESPACE = 'private'\n",
    "\n",
    "# Output directory for organized charts\n",
    "OUTPUT_DIR = 'tier-1_output/charts'\n",
    "\n",
    "# NOTE: Market/commodity pairs are now auto-parsed from uout.json\n",
    "# No need to manually specify MARKET and COMMODITY variables\n",
    "\n",
    "print(f'Configuration: {INDICATOR_NAME} from {START_DATE} to {END_DATE}')\n",
    "print(f'Output directory: {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d9660",
   "metadata": {},
   "source": [
    "## Data Fetcher Class\n",
    "\n",
    "Handles server connection and data retrieval using svr3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrinityStrategyDataFetcher:\n",
    "    \"\"\"Data fetcher for TrinityStrategy indicator using svr3 module\"\"\"\n",
    "\n",
    "    def __init__(self, token: str, start: int, end: int):\n",
    "        self.token = token\n",
    "        self.start_date = start\n",
    "        self.end_date = end\n",
    "        self.client = None\n",
    "        self.df = None\n",
    "        self.available_fields = []\n",
    "\n",
    "    async def connect(self):\n",
    "        \"\"\"Establish connection to server (Pattern 2: will update markets/codes per fetch)\"\"\"\n",
    "        print(f'üîÑ Connecting to server...')\n",
    "\n",
    "        # Initialize with dummy market/code (will be updated in fetch())\n",
    "        self.client = svr3.sv_reader(\n",
    "            self.start_date, self.end_date,\n",
    "            INDICATOR_NAME, GRANULARITY, NAMESPACE,\n",
    "            'symbol', ['DCE'], ['i<00>'],  # Dummy initial values\n",
    "            False, RAILS_URL, WS_URL,\n",
    "            '', '', TM_MASTER,\n",
    "        )\n",
    "        self.client.token = self.token\n",
    "\n",
    "        await self.client.login()\n",
    "        await self.client.connect()\n",
    "        self.client.ws_task = asyncio.create_task(self.client.ws_loop())\n",
    "        await self.client.shakehand()\n",
    "\n",
    "        print(f'‚úì Connected to server')\n",
    "\n",
    "    async def fetch(self, market: str, code: str) -> pd.DataFrame:\n",
    "        \"\"\"Fetch indicator data for specified market/code (reuses connection)\"\"\"\n",
    "        print(f'üìä Fetching indicators for {market}/{code}...')\n",
    "\n",
    "        # Update markets/codes for this fetch (Pattern 2 from wos/10-visualization.md)\n",
    "        self.client.markets = [market]\n",
    "        self.client.codes = [code]\n",
    "        self.client.namespace = NAMESPACE  # private namespace\n",
    "\n",
    "        ret = await self.client.save_by_symbol()\n",
    "        data = ret[1][1]\n",
    "\n",
    "        if not data:\n",
    "            print(f'‚ö† No indicator data returned for {market}/{code}')\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        header_fields = ['time_tag', 'granularity', 'market', 'code', 'namespace']\n",
    "        self.available_fields = [col for col in df.columns if col not in header_fields]\n",
    "\n",
    "        if 'time_tag' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['time_tag'], unit='ms')\n",
    "            df = df.sort_values('datetime')\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "        print(f'‚úì Loaded {len(df)} indicator data points')\n",
    "        if 'datetime' in df.columns:\n",
    "            print(f'  Date range: {df[\"datetime\"].min()} to {df[\"datetime\"].max()}')\n",
    "        print(f'  Available fields: {\", \".join(self.available_fields)}')\n",
    "\n",
    "        return df\n",
    "\n",
    "    async def close(self):\n",
    "        \"\"\"Clean up connection\"\"\"\n",
    "        if self.client:\n",
    "            self.client.stop()\n",
    "            await self.client.join()\n",
    "            print('‚úì Connection closed')\n",
    "\n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get summary statistics\"\"\"\n",
    "        if self.df is None or self.df.empty:\n",
    "            return {}\n",
    "\n",
    "        summary = {\n",
    "            'total_points': len(self.df),\n",
    "            'fields': self.available_fields,\n",
    "        }\n",
    "\n",
    "        if 'datetime' in self.df.columns:\n",
    "            summary['date_range'] = (self.df['datetime'].min(), self.df['datetime'].max())\n",
    "\n",
    "        numeric_fields = self.df.select_dtypes(include=[np.number]).columns\n",
    "        summary['statistics'] = {}\n",
    "        for field in numeric_fields:\n",
    "            if field not in ['time_tag', 'bar_index']:\n",
    "                summary['statistics'][field] = {\n",
    "                    'mean': self.df[field].mean(),\n",
    "                    'std': self.df[field].std(),\n",
    "                    'min': self.df[field].min(),\n",
    "                    'max': self.df[field].max()\n",
    "                }\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "async def fetch_price_data_standalone(token: str, start: int, end: int, market: str, code: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch OHLCV price data (SampleQuote) with separate dedicated connection\n",
    "    \n",
    "    This function creates its own svr3.sv_reader instance specifically for fetching\n",
    "    SampleQuote dependency data from the global namespace. Cannot reuse the indicator\n",
    "    connection because algo_name is baked into the connection initialization.\n",
    "    \n",
    "    Pattern from: IronOreIndicator/analysis.ipynb (working example)\n",
    "    \"\"\"\n",
    "    print(f'üíπ Fetching price data for {market}/{code}...')\n",
    "    \n",
    "    try:\n",
    "        # Create separate reader for SampleQuote\n",
    "        reader = svr3.sv_reader(\n",
    "            start, end,\n",
    "            'SampleQuote',      # ‚úÖ Different algo from indicator\n",
    "            GRANULARITY,\n",
    "            'global',           # ‚úÖ Global namespace for dependencies\n",
    "            'symbol',\n",
    "            [market],\n",
    "            [code],\n",
    "            False,\n",
    "            RAILS_URL, WS_URL,\n",
    "            '', '',\n",
    "            TM_MASTER,\n",
    "        )\n",
    "        reader.token = token\n",
    "        \n",
    "        # Connect\n",
    "        await reader.login()\n",
    "        await reader.connect()\n",
    "        reader.ws_task = asyncio.create_task(reader.ws_loop())\n",
    "        await reader.shakehand()\n",
    "        \n",
    "        # Fetch data\n",
    "        ret = await reader.save_by_symbol()\n",
    "        \n",
    "        # Extract data correctly: ret[1][1] is the List[Dict]\n",
    "        if ret and len(ret) > 1 and len(ret[1]) > 1:\n",
    "            data = ret[1][1]\n",
    "        else:\n",
    "            print(f'‚ö† No price data returned for {market}/{code}')\n",
    "            reader.stop()\n",
    "            await reader.join()\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Cleanup connection\n",
    "        reader.stop()\n",
    "        await reader.join()\n",
    "        \n",
    "        if not data:\n",
    "            print(f'‚ö† Empty price data for {market}/{code}')\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        if 'time_tag' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['time_tag'], unit='ms')\n",
    "            df = df.sort_values('datetime')\n",
    "        \n",
    "        print(f'‚úì Loaded {len(df)} price data points')\n",
    "        if 'datetime' in df.columns:\n",
    "            print(f'  Date range: {df[\"datetime\"].min()} to {df[\"datetime\"].max()}')\n",
    "        \n",
    "        # Check for OHLCV fields\n",
    "        expected_fields = ['open', 'high', 'low', 'close', 'volume']\n",
    "        available_price_fields = [f for f in expected_fields if f in df.columns]\n",
    "        print(f'  Price fields: {\", \".join(available_price_fields)}')\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error fetching price data: {e}')\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "print('‚úì Data fetcher class and standalone price fetcher defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9xo3ihpo6lb",
   "metadata": {},
   "source": [
    "## Plotting Functions\n",
    "\n",
    "Defines three plotting functions for generating charts:\n",
    "1. **plot_trinity_scouts**: Time series chart with all three Scout indicators\n",
    "2. **plot_distributions**: Distribution histograms for Scout indicator values\n",
    "3. **plot_correlation_matrix**: Correlation heatmap between Scout indicators\n",
    "\n",
    "All functions accept `output_path` and `title_suffix` parameters for automated multi-instrument use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc4ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trading_hours(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Filter dataframe to only include trading hours\n",
    "    \n",
    "    Chinese commodity exchange trading hours:\n",
    "    - Day session: 09:00-15:00\n",
    "    - Night session: 21:00-01:00 (next day)\n",
    "    \"\"\"\n",
    "    if df.empty or 'datetime' not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    # Extract hour and minute for precise filtering\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['minute'] = df['datetime'].dt.minute\n",
    "    \n",
    "    # Keep only trading hours:\n",
    "    # Day session: 9:00-14:59 (before 15:00)\n",
    "    # Night session: 21:00-23:59 and 00:00-00:59 (midnight hour only, not 01:xx)\n",
    "    trading_hours_mask = (\n",
    "        ((df['hour'] >= 9) & (df['hour'] < 15)) |  # Day session: 09:00-14:59\n",
    "        ((df['hour'] >= 21) & (df['hour'] <= 23)) |  # Night session: 21:00-23:59\n",
    "        (df['hour'] == 0)  # Midnight hour: 00:00-00:59 only\n",
    "    )\n",
    "    \n",
    "    filtered_df = df[trading_hours_mask].copy()\n",
    "    filtered_df.drop(['hour', 'minute'], axis=1, inplace=True)\n",
    "    \n",
    "    removed_count = len(df) - len(filtered_df)\n",
    "    if removed_count > 0:\n",
    "        print(f'  üîç Filtered: {len(df)} ‚Üí {len(filtered_df)} points (removed {removed_count} non-trading hours)')\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "print('‚úì Trading hours filter defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fh9t6rp44",
   "metadata": {},
   "outputs": [],
   "source": "def determine_resampling_freq(df: pd.DataFrame) -> tuple:\n    \"\"\"Determine appropriate resampling frequency based on data length\n    \n    Returns:\n        (resample_freq, use_candlesticks, description)\n    \n    Strategy:\n    - <= 100 bars: No resampling, full candlesticks\n    - 101-300 bars: No resampling, simplified candlesticks  \n    - 301-1000 bars: Resample to daily, line charts\n    - > 1000 bars: Resample to weekly, line charts\n    \"\"\"\n    n_bars = len(df)\n    date_range = (df['datetime'].max() - df['datetime'].min()).days\n    \n    if n_bars <= 100:\n        return None, True, f'{n_bars} bars (15-min resolution)'\n    elif n_bars <= 300:\n        return None, True, f'{n_bars} bars (15-min resolution, simplified)'\n    elif n_bars <= 1000 or date_range <= 60:\n        return 'D', False, f'{n_bars} bars ‚Üí Daily resolution ({date_range} days)'\n    else:\n        return 'W', False, f'{n_bars} bars ‚Üí Weekly resolution ({date_range} days)'\n\n\ndef resample_ohlcv_data(df: pd.DataFrame, freq: str) -> pd.DataFrame:\n    \"\"\"Resample OHLCV data to specified frequency\n    \n    Args:\n        df: DataFrame with datetime index and OHLCV columns\n        freq: Pandas frequency string ('D' for daily, 'W' for weekly)\n    \n    Returns:\n        Resampled DataFrame with OHLCV data and indicators\n    \"\"\"\n    # Set datetime as index for resampling\n    df_indexed = df.set_index('datetime')\n    \n    # Define aggregation rules\n    agg_dict = {\n        'open': 'first',\n        'high': 'max',\n        'low': 'min',\n        'close': 'last',\n        'volume': 'sum'\n    }\n    \n    # Add indicator fields with appropriate aggregation\n    indicator_fields = ['adx_value', 'di_plus', 'di_minus', \n                       'upper_band', 'middle_band', 'lower_band',\n                       'conviction_oscillator']\n    \n    for field in indicator_fields:\n        if field in df_indexed.columns:\n            agg_dict[field] = 'last'  # Use last value for indicators\n    \n    # Resample\n    resampled = df_indexed.resample(freq).agg(agg_dict).dropna()\n    \n    # Reset index to get datetime back as column\n    resampled = resampled.reset_index()\n    \n    print(f'  üìä Resampled: {len(df)} ‚Üí {len(resampled)} bars (frequency: {freq})')\n    \n    return resampled\n\n\ndef plot_trinity_scouts(df: pd.DataFrame, output_path: str, title_suffix: str = ''):\n    \"\"\"Plot the 'Golden Rule' visualization: Price + Indicators\n    \n    3-Panel Layout:\n    - Panel 1: \"Action\" - Candlesticks/Lines + Bollinger Bands (what's happening)\n    - Panel 2: \"Regime\" - ADX + DI+/DI- (how to interpret it)\n    - Panel 3: \"Confirmation\" - Conviction Oscillator (is it real?)\n    \n    Features adaptive rendering based on data length:\n    - Short periods (<= 300 bars): Full candlesticks\n    - Medium periods (301-1000 bars): Daily resampling + line charts\n    - Long periods (> 1000 bars): Weekly resampling + line charts\n    \n    Args:\n        df: DataFrame with OHLCV price data AND indicator data\n        output_path: Full path where to save the chart\n        title_suffix: Additional text to append to chart titles (e.g., market/code)\n    \"\"\"\n    if df.empty or 'datetime' not in df.columns:\n        print('‚ö† No datetime data available for plotting')\n        return\n    \n    # Check for required OHLCV fields\n    required_ohlcv = ['open', 'high', 'low', 'close', 'volume']\n    if not all(col in df.columns for col in required_ohlcv):\n        print(f'‚ö† Missing OHLCV fields for candlestick chart')\n        return\n\n    # Determine adaptive resampling strategy\n    resample_freq, use_candlesticks, strategy_desc = determine_resampling_freq(df)\n    print(f'  üìà Rendering strategy: {strategy_desc}')\n    \n    # Apply resampling if needed\n    plot_df = df.copy()\n    if resample_freq:\n        plot_df = resample_ohlcv_data(plot_df, resample_freq)\n    \n    # Calculate adaptive figure width (minimum 18, scale up for more data)\n    n_points = len(plot_df)\n    fig_width = min(max(18, n_points * 0.15), 36)  # Cap at 36 inches\n\n    # Create figure with 3 subplots\n    fig = plt.figure(figsize=(fig_width, 14))\n    \n    # Define grid: 3 rows with different heights\n    # Panel 1 (Price+BB) gets most space, Panel 2 (ADX) and Panel 3 (Conviction) smaller\n    gs = fig.add_gridspec(3, 1, height_ratios=[3, 1.5, 1.5], hspace=0.3)\n    ax1 = fig.add_subplot(gs[0])  # Candlesticks + Bollinger Bands\n    ax2 = fig.add_subplot(gs[1])  # ADX + DI+/DI-\n    ax3 = fig.add_subplot(gs[2])  # Conviction Oscillator\n    \n    # ============================================================================\n    # PANEL 1: \"ACTION\" - Price + Bollinger Bands\n    # ============================================================================\n    \n    if use_candlesticks:\n        # Plot candlesticks manually for short periods\n        price_data = plot_df[['datetime', 'open', 'high', 'low', 'close']].copy()\n        price_data = price_data.set_index('datetime')\n        \n        for idx in range(len(price_data)):\n            row = price_data.iloc[idx]\n            x = idx\n            open_price = row['open']\n            close_price = row['close']\n            high_price = row['high']\n            low_price = row['low']\n            \n            # Determine color: green if close > open (up), red if close < open (down)\n            color = 'green' if close_price >= open_price else 'red'\n            \n            # Draw high-low line (wick)\n            ax1.plot([x, x], [low_price, high_price], color='black', linewidth=0.5)\n            \n            # Draw open-close box (body)\n            height = abs(close_price - open_price)\n            bottom = min(open_price, close_price)\n            \n            if height == 0:  # Doji - draw a horizontal line\n                ax1.plot([x-0.3, x+0.3], [close_price, close_price], color=color, linewidth=1.5)\n            else:\n                rect = plt.Rectangle((x-0.3, bottom), 0.6, height, \n                                    facecolor=color, edgecolor='black', linewidth=0.5, alpha=0.8)\n                ax1.add_patch(rect)\n    else:\n        # Use line chart for longer periods (cleaner visualization)\n        x_values = plot_df.index.values\n        ax1.plot(x_values, plot_df['close'], linewidth=1.5, color='black', \n                label='Close Price', alpha=0.8)\n        \n        # Optionally show high/low range as shaded area\n        ax1.fill_between(x_values, plot_df['low'], plot_df['high'], \n                        alpha=0.1, color='gray', label='High-Low Range')\n    \n    # Overlay Bollinger Bands\n    if all(col in plot_df.columns for col in ['upper_band', 'middle_band', 'lower_band']):\n        x_values = plot_df.index.values\n        \n        # Fill between upper and lower bands (the \"stretched zone\")\n        ax1.fill_between(x_values, plot_df['lower_band'], plot_df['upper_band'], \n                         alpha=0.15, label='BB Stretched Zone', color='blue')\n        \n        # Plot band lines\n        ax1.plot(x_values, plot_df['upper_band'], linewidth=1.5, alpha=0.7, \n                color='blue', linestyle='--', label='Upper Band (\"Overpriced\")')\n        ax1.plot(x_values, plot_df['middle_band'], linewidth=2, color='orange', \n                label='Middle Band (Fair Value)')\n        ax1.plot(x_values, plot_df['lower_band'], linewidth=1.5, alpha=0.7, \n                color='blue', linestyle='--', label='Lower Band (\"On Sale\")')\n    \n    ax1.set_ylabel('Price', fontsize=11, fontweight='bold')\n    title_text = f'Panel 1: \"The Action\" - Price + Bollinger Bands - {title_suffix}' if title_suffix else 'Panel 1: \"The Action\" - Price + Bollinger Bands'\n    ax1.set_title(title_text, fontsize=12, fontweight='bold', pad=10)\n    ax1.legend(loc='upper left', fontsize=9, framealpha=0.9)\n    ax1.grid(True, alpha=0.3)\n    ax1.set_xlim(-0.5, len(plot_df) - 0.5)\n    \n    # ============================================================================\n    # PANEL 2: \"REGIME\" - ADX + Directional Movement\n    # ============================================================================\n    \n    if all(col in plot_df.columns for col in ['adx_value', 'di_plus', 'di_minus']):\n        x_values = plot_df.index.values\n        \n        ax2.plot(x_values, plot_df['adx_value'], label='ADX (Trend Strength)', \n                linewidth=2, color='black')\n        ax2.plot(x_values, plot_df['di_plus'], label='DI+ (Buyers)', \n                linewidth=1.5, alpha=0.8, color='green')\n        ax2.plot(x_values, plot_df['di_minus'], label='DI- (Sellers)', \n                linewidth=1.5, alpha=0.8, color='red')\n        \n        # Add regime threshold lines\n        ax2.axhline(y=25, color='gray', linestyle='--', alpha=0.6, \n                   label='Strong Trend (>25)', linewidth=1)\n        ax2.axhline(y=20, color='gray', linestyle=':', alpha=0.6, \n                   label='Ranging Market (<20)', linewidth=1)\n        \n        ax2.set_ylabel('ADX Value', fontsize=10, fontweight='bold')\n        title_text = f'Panel 2: \"The Regime\" - ADX Market Analyst - {title_suffix}' if title_suffix else 'Panel 2: \"The Regime\" - ADX Market Analyst'\n        ax2.set_title(title_text, fontsize=11, fontweight='bold', pad=8)\n        ax2.legend(loc='upper left', fontsize=8, framealpha=0.9, ncol=2)\n        ax2.grid(True, alpha=0.3)\n        ax2.set_xlim(-0.5, len(plot_df) - 0.5)\n    \n    # ============================================================================\n    # PANEL 3: \"CONFIRMATION\" - Volume Conviction\n    # ============================================================================\n    \n    if 'conviction_oscillator' in plot_df.columns:\n        x_values = plot_df.index.values\n        \n        # Color bars based on sign (green = bullish, red = bearish)\n        colors = ['green' if x > 0 else 'red' for x in plot_df['conviction_oscillator']]\n        ax3.bar(x_values, plot_df['conviction_oscillator'], color=colors, alpha=0.6, width=0.8)\n        ax3.axhline(y=0, color='black', linestyle='-', linewidth=1)\n        \n        ax3.set_ylabel('Conviction', fontsize=10, fontweight='bold')\n        title_text = f'Panel 3: \"The Confirmation\" - Volume Analyst - {title_suffix}' if title_suffix else 'Panel 3: \"The Confirmation\" - Volume Analyst'\n        ax3.set_title(title_text, fontsize=11, fontweight='bold', pad=8)\n        ax3.grid(True, alpha=0.3)\n        ax3.set_xlim(-0.5, len(plot_df) - 0.5)\n    \n    # ============================================================================\n    # X-AXIS FORMATTING (Adaptive based on data density)\n    # ============================================================================\n    \n    total_points = len(plot_df)\n    \n    # Calculate optimal tick spacing based on total points\n    if total_points <= 20:\n        tick_spacing = 1\n        date_fmt = '%m/%d\\n%H:%M'\n    elif total_points <= 50:\n        tick_spacing = max(3, total_points // 15)\n        date_fmt = '%m/%d\\n%H:%M'\n    elif total_points <= 150:\n        tick_spacing = max(8, total_points // 14)\n        date_fmt = '%m/%d\\n%H:%M'\n    elif total_points <= 365:\n        tick_spacing = max(15, total_points // 20)\n        date_fmt = '%Y-%m-%d'  # Daily format\n    else:\n        tick_spacing = max(20, total_points // 25)\n        date_fmt = '%Y-%m-%d'  # Weekly/Monthly format\n    \n    # Generate tick positions\n    tick_positions = list(range(0, total_points, tick_spacing))\n    if tick_positions[-1] != total_points - 1:\n        tick_positions.append(total_points - 1)\n    \n    # Generate datetime labels\n    tick_labels = []\n    for pos in tick_positions:\n        dt = plot_df['datetime'].iloc[pos]\n        label = dt.strftime(date_fmt)\n        tick_labels.append(label)\n    \n    # Apply to all axes\n    for ax in [ax1, ax2, ax3]:\n        ax.set_xticks(tick_positions)\n        ax.set_xticklabels(tick_labels, fontsize=8, ha='center', rotation=45 if total_points > 150 else 0)\n        \n        # Only bottom panel gets x-label\n        if ax == ax3:\n            ax.set_xlabel('Trading Date & Time', fontsize=11, fontweight='bold', labelpad=8)\n    \n    # Add overall title with resolution info\n    resolution_text = f'({strategy_desc})'\n    fig.suptitle(f'Trinity Strategy Visualization {resolution_text}', \n                 fontsize=14, fontweight='bold', y=0.995)\n    \n    # Save figure\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close(fig)\n\n\ndef plot_distributions(df: pd.DataFrame, fields: List[str], output_path: str, title_suffix: str = ''):\n    \"\"\"Plot distributions for Scout fields\"\"\"\n    if df.empty:\n        print('‚ö† No data available for plotting distributions')\n        return\n\n    # Filter to only Scout indicator fields (not header fields)\n    scout_fields = ['adx_value', 'di_plus', 'di_minus', 'upper_band', 'middle_band', \n                    'lower_band', 'conviction_oscillator']\n    plot_fields = [f for f in scout_fields if f in df.columns and pd.api.types.is_numeric_dtype(df[f])]\n\n    if not plot_fields:\n        print('‚ö† No Scout fields available for distribution plotting')\n        return\n\n    n_plots = len(plot_fields)\n    n_cols = 3\n    n_rows = (n_plots + 2) // 3\n\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n    axes = axes.flatten() if n_plots > 1 else [axes]\n\n    for idx, field in enumerate(plot_fields):\n        data = df[field].dropna()\n        axes[idx].hist(data, bins=30, alpha=0.7, edgecolor='black', color='steelblue')\n        axes[idx].set_xlabel(field, fontsize=9)\n        axes[idx].set_ylabel('Frequency', fontsize=9)\n        axes[idx].set_title(f'{field}', fontsize=10, fontweight='bold')\n        axes[idx].grid(True, alpha=0.3)\n\n        mean_val = data.mean()\n        axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, \n                         label=f'Mean: {mean_val:.4f}')\n        axes[idx].legend(fontsize=8)\n\n    # Hide unused subplots\n    for idx in range(len(plot_fields), len(axes)):\n        axes[idx].axis('off')\n\n    # Add overall title\n    fig.suptitle(f'Distribution Analysis - {title_suffix}' if title_suffix else 'Distribution Analysis', \n                 fontsize=12, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close(fig)\n\n\ndef plot_correlation_matrix(df: pd.DataFrame, fields: List[str], output_path: str, title_suffix: str = ''):\n    \"\"\"Plot correlation matrix for Scout fields\"\"\"\n    if df.empty:\n        print('‚ö† No data available for correlation analysis')\n        return\n\n    # Filter to only Scout indicator fields\n    scout_fields = ['adx_value', 'di_plus', 'di_minus', 'upper_band', 'middle_band', \n                    'lower_band', 'conviction_oscillator']\n    numeric_fields = [f for f in scout_fields if f in df.columns and pd.api.types.is_numeric_dtype(df[f])]\n\n    if len(numeric_fields) < 2:\n        print('‚ö† Need at least 2 Scout fields for correlation analysis')\n        return\n\n    corr_df = df[numeric_fields].corr()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_df, annot=True, fmt='.3f', cmap='coolwarm', center=0,\n                square=True, linewidths=1, cbar_kws={'shrink': 0.8})\n    \n    title_text = f'Scout Correlation Matrix - {title_suffix}' if title_suffix else 'Scout Correlation Matrix'\n    plt.title(title_text, fontsize=12, fontweight='bold')\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.close()\n\n\nprint('‚úì All plotting functions defined (plot_trinity_scouts, plot_distributions, plot_correlation_matrix)')"
  },
  {
   "cell_type": "markdown",
   "id": "233eae0f",
   "metadata": {},
   "source": [
    "## Automated Multi-Instrument Visualization\n",
    "\n",
    "Connects to server and generates all charts for every instrument configured in `uout.json`.\n",
    "\n",
    "**Process:**\n",
    "1. Parse instruments from `uout.json` ‚Üí `[('DCE', 'i<00>'), ('SHFE', 'cu<00>'), ('SHFE', 'sc<00>')]`\n",
    "2. Connect once to server (efficient Pattern 2 from wos/10-visualization.md)\n",
    "3. For each instrument:\n",
    "   - Fetch data (reusing connection)\n",
    "   - Generate 3 chart types ‚Üí save to `tier-1_output/charts/<instrument>/`\n",
    "4. Close connection\n",
    "\n",
    "**Output Structure:**\n",
    "```\n",
    "tier-1_output/charts/\n",
    "‚îú‚îÄ‚îÄ i/       (Iron Ore - DCE)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ trinity_scouts_DATES.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ trinity_distributions_DATES.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ trinity_correlation_DATES.png\n",
    "‚îú‚îÄ‚îÄ cu/      (Copper - SHFE)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ trinity_scouts_DATES.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ trinity_distributions_DATES.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ trinity_correlation_DATES.png\n",
    "‚îî‚îÄ‚îÄ sc/      (Crude Oil - SHFE)\n",
    "    ‚îú‚îÄ‚îÄ trinity_scouts_DATES.png\n",
    "    ‚îú‚îÄ‚îÄ trinity_distributions_DATES.png\n",
    "    ‚îî‚îÄ‚îÄ trinity_correlation_DATES.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60886ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse instruments from uout.json\n",
    "instruments = parse_uout_instruments()\n",
    "print(f'üìä Found {len(instruments)} instruments to visualize:')\n",
    "for market, code in instruments:\n",
    "    print(f'   - {market}/{code}')\n",
    "\n",
    "# Create output directory structure\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f'\\nüìÅ Output directory: {OUTPUT_DIR}')\n",
    "\n",
    "# Initialize fetcher and connect once (for indicators)\n",
    "fetcher = TrinityStrategyDataFetcher(TOKEN, START_DATE, END_DATE)\n",
    "await fetcher.connect()\n",
    "\n",
    "# Process each instrument\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print('Starting automated visualization for all instruments...')\n",
    "print(f'{\"=\"*70}\\n')\n",
    "\n",
    "for market, code in instruments:\n",
    "    print(f'{\"=\"*70}')\n",
    "    print(f'üìà Processing {market}/{code}...')\n",
    "    print(f'{\"=\"*70}')\n",
    "    \n",
    "    # Fetch indicator data (private namespace - reuses fetcher connection)\n",
    "    indicator_df = await fetcher.fetch(market, code)\n",
    "    \n",
    "    if indicator_df.empty:\n",
    "        print(f'‚ö† No indicator data returned for {market}/{code}\\n')\n",
    "        continue\n",
    "    \n",
    "    # Fetch price data (global namespace - SEPARATE connection)\n",
    "    # ‚úÖ FIX: Use standalone function with its own connection\n",
    "    price_df = await fetch_price_data_standalone(TOKEN, START_DATE, END_DATE, market, code)\n",
    "    \n",
    "    if price_df.empty:\n",
    "        print(f'‚ö† No price data returned for {market}/{code}\\n')\n",
    "        continue\n",
    "    \n",
    "    # Merge price and indicator data on time_tag\n",
    "    print(f'üîó Merging price and indicator data...')\n",
    "    merged_df = pd.merge(\n",
    "        price_df, \n",
    "        indicator_df,\n",
    "        on='time_tag',\n",
    "        how='inner',  # Only keep timestamps where both exist\n",
    "        suffixes=('_price', '_indicator')\n",
    "    )\n",
    "    \n",
    "    # Use datetime from price data (they should be identical)\n",
    "    if 'datetime_price' in merged_df.columns:\n",
    "        merged_df['datetime'] = merged_df['datetime_price']\n",
    "        merged_df.drop(['datetime_price', 'datetime_indicator'], axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Reset index for continuous plotting\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f'‚úì Merged dataset: {len(merged_df)} data points')\n",
    "    print(f'  Date range: {merged_df[\"datetime\"].min()} to {merged_df[\"datetime\"].max()}')\n",
    "    \n",
    "    # Check for required fields\n",
    "    required_ohlcv = ['open', 'high', 'low', 'close', 'volume']\n",
    "    required_indicators = ['upper_band', 'middle_band', 'lower_band', 'adx_value', 'conviction_oscillator']\n",
    "    \n",
    "    missing_fields = [f for f in required_ohlcv + required_indicators if f not in merged_df.columns]\n",
    "    if missing_fields:\n",
    "        print(f'‚ö† Missing required fields: {\", \".join(missing_fields)}')\n",
    "        print(f'  Skipping {market}/{code}\\n')\n",
    "        continue\n",
    "    \n",
    "    # Update fetcher's dataframe for distribution/correlation plots\n",
    "    fetcher.df = merged_df\n",
    "    \n",
    "    # Create instrument-specific subdirectory\n",
    "    instrument_name = code.replace('<00>', '')  # 'i<00>' -> 'i'\n",
    "    chart_dir = os.path.join(OUTPUT_DIR, instrument_name)\n",
    "    os.makedirs(chart_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate all three chart types\n",
    "    title_suffix = f'{market}/{code}'\n",
    "    \n",
    "    # 1. Scout time series chart WITH PRICE CANDLESTICKS (NEW!)\n",
    "    scouts_file = os.path.join(chart_dir, f'trinity_scouts_{START_DATE}_{END_DATE}.png')\n",
    "    plot_trinity_scouts(merged_df, scouts_file, title_suffix)\n",
    "    print(f'‚úì Saved: {scouts_file}')\n",
    "    \n",
    "    # 2. Distribution analysis chart\n",
    "    dist_file = os.path.join(chart_dir, f'trinity_distributions_{START_DATE}_{END_DATE}.png')\n",
    "    plot_distributions(merged_df, fetcher.available_fields, dist_file, title_suffix)\n",
    "    print(f'‚úì Saved: {dist_file}')\n",
    "    \n",
    "    # 3. Correlation matrix chart\n",
    "    corr_file = os.path.join(chart_dir, f'trinity_correlation_{START_DATE}_{END_DATE}.png')\n",
    "    plot_correlation_matrix(merged_df, fetcher.available_fields, corr_file, title_suffix)\n",
    "    print(f'‚úì Saved: {corr_file}')\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Close connection (cleanup)\n",
    "await fetcher.close()\n",
    "\n",
    "print(f'{\"=\"*70}')\n",
    "print('‚úÖ All visualizations complete!')\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'\\nüìÅ Charts saved to: {OUTPUT_DIR}/')\n",
    "print(f'   Directory structure:')\n",
    "for market, code in instruments:\n",
    "    instrument_name = code.replace('<00>', '')\n",
    "    print(f'   ‚îú‚îÄ‚îÄ {instrument_name}/')\n",
    "    print(f'   ‚îÇ   ‚îú‚îÄ‚îÄ trinity_scouts_{START_DATE}_{END_DATE}.png')\n",
    "    print(f'   ‚îÇ   ‚îú‚îÄ‚îÄ trinity_distributions_{START_DATE}_{END_DATE}.png')\n",
    "    print(f'   ‚îÇ   ‚îî‚îÄ‚îÄ trinity_correlation_{START_DATE}_{END_DATE}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}